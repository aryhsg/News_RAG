import httpx
from fastapi import FastAPI
from fastapi.concurrency import run_in_threadpool
from typing import List, Dict, Any
import os
from typing import List, Dict, Any, Optional
from supabase import create_client, Client
from dotenv import load_dotenv
from google import genai
from google.genai.errors import APIError
from postgrest.exceptions import APIError as PostgrestAPIError # æ•ç² Supabase éŒ¯èª¤
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import time
from datetime import datetime, timezone, timedelta


# =======================================================================================
load_dotenv()
# 1. å®šç¾© UTC+8 æ™‚å€ # å°ç£æ™‚é–“æ˜¯ UTC+8
tz_taipei = timezone(timedelta(hours=8))

# 2. å–å¾—ç¾åœ¨æ™‚é–“ä¸¦æŒ‡å®šæ™‚å€
taipei_date = datetime.now(tz=tz_taipei).date()

CHUNK_SIZE = 300
CHUNK_OVERLAP = 50

# =======================================================================================
def split_content_fixed(news_content_dict: Dict[str, str], metadata_dict: Dict[str, Any]) -> List[Document]:
    text_spliter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "ã€‚", "?", "!", " ", "", "."],
        length_function=len,
        is_separator_regex=False
    )

    # ä¿®æ­£å‚³åƒï¼šç›´æ¥å‚³é content æ–‡æœ¬å’Œ metadata å­—å…¸
    chunks = text_spliter.create_documents(
        texts=[news_content_dict["content"]],
        metadatas=[metadata_dict]
    )

    for i, doc in enumerate(chunks):
        doc.metadata['chunk_index'] = i

    return chunks
# =======================================================================================
def transform_datastruct_and_split(news_data):
    news_urls = news_data["data"]["url"]
    news_titles = news_data["data"]["title"]
    news_contents = news_data["data"]["content"]
    
    # æ›´æ”¹ï¼šnews_list ç¾åœ¨ç”¨æ–¼æ”¶é›†æ‰€æœ‰æ–‡ç« çš„æ‰€æœ‰åˆ†å¡Š (Document ç‰©ä»¶)
    all_chunks_list: List[Document] = [] 
    
    for i in range(len(news_urls)):
        # 1. æº–å‚™ news å’Œ metadata
        news_content_dict = {"content": news_contents[i]}
        metadata_dict = {
            "date": taipei_date.isoformat(), # å»ºè­°å°‡ date è½‰ç‚ºå­—ä¸²ä»¥ç¢ºä¿ JSON/Dict å…¼å®¹æ€§
            "url": news_urls[i],
            "title": news_titles[i]
        }
        
        # 2. åŸ·è¡Œåˆ‡å¡Š (å›å‚³ List[Document])
        # æˆ‘å€‘éœ€è¦å°‡å…§å®¹å’Œ metadata åˆ†é–‹å‚³éçµ¦ split_content
        # ä¿®æ­£ split_content å‚³åƒæ–¹å¼ï¼Œæ›´æ¸…æ™°
        news_chunks: List[Document] = split_content_fixed(news_content_dict, metadata_dict)
        
        # 3. ã€é—œéµä¿®æ­£é»ã€‘ä½¿ç”¨ extend() å°‡æ‰€æœ‰åˆ†å¡Š Document åŠ å…¥ç¸½åˆ—è¡¨
        all_chunks_list.extend(news_chunks)
        
    # 4. è½‰æ›ç‚º List[Dict] çµæ§‹ (å°‡ Document å±•é–‹)
    # é€™ä¸€éƒ¨åˆ†ç§»åˆ°é€™è£¡åŸ·è¡Œï¼Œå¯ä»¥é¿å…åœ¨è¿´åœˆå…§åè¦†æ“ä½œ
    final_list = [
        {"content": doc.page_content, "metadata": doc.metadata}
        for doc in all_chunks_list
    ]

    return final_list
# =======================================================================================
class SupabaseUploader:
    """
    è² è²¬åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯å’Œ Supabase å®¢æˆ¶ç«¯ï¼Œ
    ä¸¦åŸ·è¡Œæ‰¹é‡ Embedding è½‰æ›å’Œè³‡æ–™åº«å¯«å…¥çš„é¡åˆ¥ã€‚
    """
    def __init__(self, embedding_model: str = 'text-embedding-004'):
        """åˆå§‹åŒ–å®¢æˆ¶ç«¯ä¸¦æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ã€‚"""
        
        # 1. åˆå§‹åŒ– Supabase å®¢æˆ¶ç«¯
        url: str = os.environ.get("SUPABASE_URL")
        # å‡è¨­æ‚¨çš„ .env æª”æ¡ˆä¸­çš„å¯†é‘°æ˜¯å« SUPABASE_KEY æˆ– SUPABASE_SERVICE_KEY
        # è«‹æ ¹æ“šæ‚¨çš„å¯¦éš› .env è®Šæ•¸åç¨±ä¿®æ”¹ä¸‹é¢é€™è¡Œï¼š
        key: str = os.environ.get("SUPABASE_KEY") or os.environ.get("password") 
        
        if not url or not key:
            raise ValueError(
                "Supabase ç’°å¢ƒè®Šæ•¸ (URL æˆ– Key) è®€å–å¤±æ•—ã€‚è«‹æª¢æŸ¥ .env æª”æ¡ˆã€‚"
            )
            
        self.supabase: Client = create_client(url, key)
        self.embedding_model = embedding_model
        self.gemini_client: Optional[genai.Client] = None

        # 2. åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯
        try:
            api_key = os.environ.get("GEMINI_API_KEY")
            if not api_key:
                 raise ValueError("GEMINI_API_KEY è®Šæ•¸æœªè¨­å®šã€‚")

            self.gemini_client = genai.Client(api_key=api_key)
            print("ğŸŸ¢ Gemini å’Œ Supabase å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")

        except Exception as e:
            print(f"ğŸ”´ éŒ¯èª¤ï¼šGemini å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
            self.gemini_client = None


    def _transform_to_vector(self, contents: List[str]) -> List[List[float]]:
            """å…§éƒ¨æ–¹æ³•ï¼šæ‰¹é‡å‘¼å« Gemini API è½‰æ›æ–‡æœ¬ç‚ºå‘é‡ï¼Œä¸¦è™•ç† API çš„æ‰¹æ¬¡é™åˆ¶ã€‚"""
            if not self.gemini_client:
                return []
                
            MAX_BATCH_SIZE = 100  # Gemini API çš„é™åˆ¶
            all_vectors: List[List[float]] = []
            total_contents = len(contents)
            
            print(f"-> ç¸½å…±æœ‰ {total_contents} ç¯‡æ–‡æœ¬éœ€è¦è½‰æ›ã€‚")

            # ä½¿ç”¨è¿´åœˆå°‡ç¸½å…§å®¹åˆ†å‰²æˆå¤šå€‹å°æ‰¹æ¬¡
            for i in range(0, total_contents, MAX_BATCH_SIZE):
                # ç¢ºå®šç•¶å‰æ‰¹æ¬¡çš„èµ·å§‹å’ŒçµæŸç´¢å¼•
                batch_contents = contents[i:i + MAX_BATCH_SIZE]
                batch_num = (i // MAX_BATCH_SIZE) + 1
                print(f"-> æ­£åœ¨è™•ç†æ‰¹æ¬¡ {batch_num} (æ•¸é‡: {len(batch_contents)})...")

                try:
                    response = self.gemini_client.models.embed_content(
                        model=self.embedding_model,
                        contents=batch_contents
                    )
                except APIError as e:
                    print(f"ğŸ”´ éŒ¯èª¤ï¼šæ‰¹æ¬¡ {batch_num} Gemini API å‘¼å«å¤±æ•—: {e}")
                    # å¦‚æœæŸå€‹æ‰¹æ¬¡å¤±æ•—ï¼Œæ‚¨å¯ä»¥é¸æ“‡è·³éè©²æ‰¹æ¬¡æˆ–ç›´æ¥è¿”å›
                    return [] 
                
                # å°‡ç•¶å‰æ‰¹æ¬¡çš„å‘é‡åŠ å…¥ç¸½åˆ—è¡¨
                batch_vectors: List[List[float]] = [
                    e.values for e in response.embeddings if hasattr(e, 'values')
                ]
                
                if len(batch_vectors) != len(batch_contents):
                    print(f"âš ï¸ è­¦å‘Š: æ‰¹æ¬¡ {batch_num} çš„å‘é‡æ•¸é‡ ({len(batch_vectors)}) èˆ‡è¼¸å…¥æ–‡æœ¬æ•¸é‡ ({len(batch_contents)}) ä¸ç¬¦ã€‚")
                    # é€™è£¡å¯èƒ½éœ€è¦æ›´åš´æ ¼çš„éŒ¯èª¤è™•ç†ï¼Œä»¥ç¢ºä¿æ•¸æ“šå’Œå‘é‡æ˜¯å°é½Šçš„ã€‚
                
                all_vectors.extend(batch_vectors)

            print(f"-> æˆåŠŸè½‰æ› {len(all_vectors)} å€‹å‘é‡ï¼ˆåˆ† {batch_num} æ‰¹æ¬¡ï¼‰ã€‚")
            return all_vectors
    
    def upload_data(self, news_data: List[list[str, dict]], table_name: str = "news") -> None: # (self, news_data: List[Dict[str, str]], table_name: str = "news")
        """
        ä¸»æ–¹æ³•ï¼šåŸ·è¡Œæ•´å€‹æµç¨‹ï¼Œå°‡æ–°èè³‡æ–™è½‰æ›ç‚ºå‘é‡ä¸¦æ’å…¥è³‡æ–™åº«ã€‚

        åƒæ•¸:
        - news_data: å¤–éƒ¨åŒ¯å…¥çš„åŸå§‹æ–°èè³‡æ–™åˆ—è¡¨ [{title, content, url}, ...]
        - table_name: è¦å¯«å…¥çš„ Supabase è¡¨æ ¼åç¨± (é è¨­ç‚º 'news')
        """
        if not self.gemini_client:
            print("ğŸ”´ ç„¡æ³•åŸ·è¡Œä¸Šå‚³ï¼ŒGemini å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ã€‚")
            return

        # 1. æº–å‚™è¼¸å…¥æ–‡æœ¬åˆ—è¡¨ (Contents for Embedding)
        contents = [news["content"] for news in news_data]

        # 2. è½‰æ›æ‰€æœ‰æ–‡æœ¬ç‚ºå‘é‡
        vectors_list = self._transform_to_vector(contents)
        
        if not vectors_list:
            print("ğŸ”´ å‘é‡è½‰æ›å¤±æ•—æˆ–å›å‚³ç‚ºç©ºï¼Œåœæ­¢å¯«å…¥è³‡æ–™åº«ã€‚")
            return

        # 3. æº–å‚™æœ€çµ‚æ’å…¥è³‡æ–™åº«çš„è¡Œåˆ—è¡¨
        insert_rows: List[Dict[str, Any]] = []
        
        for i, news in enumerate(news_data):
            # ç¢ºä¿æœ‰å°æ‡‰çš„å‘é‡
            if i < len(vectors_list):
                # å»ºç«‹è¦æ’å…¥çš„å–®è¡Œè³‡æ–™å­—å…¸
                insert_row = {
                    "url": news["metadata"]["url"],
                    "chunk_index": news["metadata"]["chunk_index"],
                    "content": news["content"],
                    "metadata": news["metadata"],
                    "embedding": vectors_list[i] # æ’å…¥ List[float]
                }
                insert_rows.append(insert_row)
            
        # 4. æ‰¹é‡æ’å…¥ Supabase
        print(f"-> å˜—è©¦æ‰¹é‡æ’å…¥ {len(insert_rows)} ç­†è³‡æ–™åˆ°è¡¨æ ¼ '{table_name}'...")
        try:
            supa_response = (
                self.supabase.table(table_name)
                .upsert(insert_rows, on_conflict="url,chunk_index")
                .execute()
            )
            # Supabase SDK å›å‚³çš„ response æ˜¯ä¸€å€‹ PostgrestAPIResponse ç‰©ä»¶
            print(f"ğŸŸ¢ è³‡æ–™åº«å¯«å…¥æˆåŠŸï¼")
            
        except PostgrestAPIError as e:
            # æ•ç²å¸¸è¦‹çš„ Postgrest éŒ¯èª¤ï¼Œä¾‹å¦‚ä¸»éµè¡çª (url unique é™åˆ¶)
            print(f"ğŸ”´ è³‡æ–™åº«å¯«å…¥å¤±æ•— (Postgrest Error): {e}")
        except Exception as e:
            print(f"ğŸ”´ è³‡æ–™åº«å¯«å…¥å¤±æ•— (æœªçŸ¥éŒ¯èª¤): {e}")

# =======================================================================================
# Buildpacks æœƒæŸ¥æ‰¾é€™å€‹ 'app' ç‰©ä»¶
app = FastAPI()
BASE_PATH = "https://aryhsgsnewsapi.onrender.com/api/scrape-specific-news/?category="
supa_client = SupabaseUploader()
category_list = ["é‡‘è","ç†è²¡","æœŸè²¨","è­‰åˆ¸","ç”¢æ¥­","åœ‹éš›"]

@app.get("/", status_code=200, summary="çˆ¬å–ä¸¦ä¸Šå‚³è²¡ç¶“ç›¸é—œé¡åˆ¥çš„æ–°èæ•¸æ“š")
async def curl_news_and_upload():
    """
    æ¥æ”¶ Web è«‹æ±‚å¾Œï¼Œç•°æ­¥çˆ¬å–æ‰€æœ‰æŒ‡å®šé¡åˆ¥çš„æ–°èæ•¸æ“šï¼Œ
    å°‡æ•¸æ“šè½‰æ›å¾Œï¼Œä¸Šå‚³åˆ° Supabase çš„ 'news' è¡¨æ ¼ä¸­ã€‚
    """
    response_list: List[List[Dict[str, Any]]] = []

    async with httpx.AsyncClient(timeout=100.0) as client:
        try:
            for cate in category_list:
                print(f"æ­£åœ¨çˆ¬å–{cate}é¡æ–‡ç« ...")
                url = BASE_PATH+cate
                response = await client.get(url)
                response.raise_for_status()

                transformed_data = transform_datastruct_and_split(response.json())
                response_list.append(transformed_data)
                print(f"{cate}é¡æ–‡ç« çˆ¬å–å®Œæˆ!!!")

            #all_response = response_list[0]+response_list[1]+response_list[2]+response_list[3]+response_list[4]+response_list[5]
            all_response = sum(response_list, [])
        
            await run_in_threadpool(supa_client.upload_data,news_data=all_response, table_name="news")
            return {
                "message": "æ–°èæ•¸æ“šçˆ¬å–ã€å‘é‡è½‰æ›å’Œä¸Šå‚³æˆåŠŸ",
                "total_records_processed": len(all_response)
            }
            
        except ValueError as e:
            print(f"ğŸ”´ è‡´å‘½éŒ¯èª¤ï¼šç’°å¢ƒè¨­å®šå•é¡Œ: {e}")

        except Exception as e:
            print(f"ğŸ”´ é‹è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")

        except IndexError as e:
            print(f"ğŸ”´ çˆ¬å–æ–°èæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")

        except httpx.HTTPStatusError as e:
            error_message = f"ğŸ”´ å¤–éƒ¨ API è«‹æ±‚å¤±æ•— (ç‹€æ…‹ç¢¼: {e.response.status_code}): {e.response.text}"
            print(error_message)
            return {"error": "HTTP è«‹æ±‚å¤±æ•—", "details": error_message}



@app.get("/awake/")
def read_root():
    return {"status": "OK"}
    

    