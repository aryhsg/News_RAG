import httpx
from fastapi import FastAPI
from fastapi.concurrency import run_in_threadpool
from typing import List, Dict, Any
import os
from typing import List, Dict, Any, Optional
from supabase import create_client, Client
from dotenv import load_dotenv
from google import genai
from google.genai.errors import APIError
from postgrest.exceptions import APIError as PostgrestAPIError # æ•ç² Supabase éŒ¯èª¤
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import time
from datetime import datetime, timezone, timedelta


# =======================================================================================
load_dotenv()
# 1. å®šç¾© UTC+8 æ™‚å€ # å°ç£æ™‚é–“æ˜¯ UTC+8
tz_taipei = timezone(timedelta(hours=8))

# 2. å–å¾—ç¾åœ¨æ™‚é–“ä¸¦æŒ‡å®šæ™‚å€
taipei_date = datetime.now(tz=tz_taipei).date()

CHUNK_SIZE = 300
CHUNK_OVERLAP = 50

# =======================================================================================
def split_content(news):
    text_spliter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "ã€‚", "?", "!", " ", "", "."],
        length_function=len,
        is_separator_regex=False
        )

    chunks = text_spliter.create_documents(
        texts=[news[0]["content"]],
        metadatas=[news[1]]
    )
    return chunks

# =======================================================================================
def transform_datastruct_and_split(news_data):
    news_urls = news_data["data"]["url"]
    news_titles = news_data["data"]["title"]
    news_contents = news_data["data"]["content"]
    news_list = []
    for i in range(len(news_urls)):
        # news = [news:dict, metadata:dict]
        news = [
            {
            "content": news_contents[i]
            },
            {
            "date": taipei_date,
            "url": news_urls[i],
            "title": news_titles[i]
            }
                ]
        news_chunks = split_content(news) # news_chunks = [[page_content,metadata], [page_content,metadata], [page_content,metadata], ...]
        news = {"content": news_chunks[0].page_content, "metadata": news_chunks[1].metadata}
        #news_list.append(news_chunks) # new_list = [news_chunks, news_chunks, news_chunks, ...]
        #all_news_chunks = sum(news_chunks, []) # æ‰€æœ‰news_chunksé›†åˆæˆä¸€å€‹åˆ—è¡¨ [[page_content,metadata], [page_content,metadata], [page_content,metadata], ...]
        news_list.append(news)
    return news_list

# =======================================================================================
class SupabaseUploader:
    """
    è² è²¬åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯å’Œ Supabase å®¢æˆ¶ç«¯ï¼Œ
    ä¸¦åŸ·è¡Œæ‰¹é‡ Embedding è½‰æ›å’Œè³‡æ–™åº«å¯«å…¥çš„é¡åˆ¥ã€‚
    """
    def __init__(self, embedding_model: str = 'text-embedding-004'):
        """åˆå§‹åŒ–å®¢æˆ¶ç«¯ä¸¦æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ã€‚"""
        
        # 1. åˆå§‹åŒ– Supabase å®¢æˆ¶ç«¯
        url: str = os.environ.get("SUPABASE_URL")
        # å‡è¨­æ‚¨çš„ .env æª”æ¡ˆä¸­çš„å¯†é‘°æ˜¯å« SUPABASE_KEY æˆ– SUPABASE_SERVICE_KEY
        # è«‹æ ¹æ“šæ‚¨çš„å¯¦éš› .env è®Šæ•¸åç¨±ä¿®æ”¹ä¸‹é¢é€™è¡Œï¼š
        key: str = os.environ.get("SUPABASE_KEY") or os.environ.get("password") 
        
        if not url or not key:
            raise ValueError(
                "Supabase ç’°å¢ƒè®Šæ•¸ (URL æˆ– Key) è®€å–å¤±æ•—ã€‚è«‹æª¢æŸ¥ .env æª”æ¡ˆã€‚"
            )
            
        self.supabase: Client = create_client(url, key)
        self.embedding_model = embedding_model
        self.gemini_client: Optional[genai.Client] = None

        # 2. åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯
        try:
            api_key = os.environ.get("GEMINI_API_KEY")
            if not api_key:
                 raise ValueError("GEMINI_API_KEY è®Šæ•¸æœªè¨­å®šã€‚")

            self.gemini_client = genai.Client(api_key=api_key)
            print("ğŸŸ¢ Gemini å’Œ Supabase å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")

        except Exception as e:
            print(f"ğŸ”´ éŒ¯èª¤ï¼šGemini å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
            self.gemini_client = None


    def _transform_to_vector(self, contents: List[str]) -> List[List[float]]:
        """å…§éƒ¨æ–¹æ³•ï¼šæ‰¹é‡å‘¼å« Gemini API è½‰æ›æ–‡æœ¬ç‚ºå‘é‡ã€‚"""
        if not self.gemini_client:
            return []
            
        print(f"-> æ­£åœ¨å‘¼å« Gemini API è½‰æ› {len(contents)} ç¯‡æ–‡æœ¬...")
        
        try:
            response = self.gemini_client.models.embed_content(
                model=self.embedding_model,
                contents=contents
            )
        except APIError as e:
            print(f"ğŸ”´ éŒ¯èª¤ï¼šGemini API å‘¼å«å¤±æ•—: {e}")
            return []
        
        # æå– 'values' åˆ—è¡¨ä¸¦å›å‚³
        # ç¢ºä¿å›å‚³çš„æ˜¯ List[List[float]]
        vectors: List[List[float]] = [
            e.values for e in response.embeddings if hasattr(e, 'values')
        ]
        
        if len(vectors) != len(contents):
             print(f"âš ï¸ è­¦å‘Š: å‘é‡æ•¸é‡ ({len(vectors)}) èˆ‡è¼¸å…¥æ–‡æœ¬æ•¸é‡ ({len(contents)}) ä¸ç¬¦ã€‚")

        print(f"-> æˆåŠŸè½‰æ› {len(vectors)} å€‹å‘é‡ã€‚")
        return vectors

    
    def upload_data(self, news_data: List[list[str, dict]], table_name: str = "news") -> None: # (self, news_data: List[Dict[str, str]], table_name: str = "news")
        """
        ä¸»æ–¹æ³•ï¼šåŸ·è¡Œæ•´å€‹æµç¨‹ï¼Œå°‡æ–°èè³‡æ–™è½‰æ›ç‚ºå‘é‡ä¸¦æ’å…¥è³‡æ–™åº«ã€‚

        åƒæ•¸:
        - news_data: å¤–éƒ¨åŒ¯å…¥çš„åŸå§‹æ–°èè³‡æ–™åˆ—è¡¨ [{title, content, url}, ...]
        - table_name: è¦å¯«å…¥çš„ Supabase è¡¨æ ¼åç¨± (é è¨­ç‚º 'news')
        """
        if not self.gemini_client:
            print("ğŸ”´ ç„¡æ³•åŸ·è¡Œä¸Šå‚³ï¼ŒGemini å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ã€‚")
            return

        # 1. æº–å‚™è¼¸å…¥æ–‡æœ¬åˆ—è¡¨ (Contents for Embedding)
        contents = [news["content"] for news in news_data]

        # 2. è½‰æ›æ‰€æœ‰æ–‡æœ¬ç‚ºå‘é‡
        vectors_list = self._transform_to_vector(contents)
        
        if not vectors_list:
            print("ğŸ”´ å‘é‡è½‰æ›å¤±æ•—æˆ–å›å‚³ç‚ºç©ºï¼Œåœæ­¢å¯«å…¥è³‡æ–™åº«ã€‚")
            return

        # 3. æº–å‚™æœ€çµ‚æ’å…¥è³‡æ–™åº«çš„è¡Œåˆ—è¡¨
        insert_rows: List[Dict[str, Any]] = []
        
        for i, news in enumerate(news_data):
            # ç¢ºä¿æœ‰å°æ‡‰çš„å‘é‡
            if i < len(vectors_list):
                # å»ºç«‹è¦æ’å…¥çš„å–®è¡Œè³‡æ–™å­—å…¸
                insert_row = {
                    "url": news["metadata"]["url"],
                    "content": news["content"],
                    "metadata": news["metadata"],
                    "embedding": vectors_list[i] # æ’å…¥ List[float]
                }
                insert_rows.append(insert_row)
            
        # 4. æ‰¹é‡æ’å…¥ Supabase
        print(f"-> å˜—è©¦æ‰¹é‡æ’å…¥ {len(insert_rows)} ç­†è³‡æ–™åˆ°è¡¨æ ¼ '{table_name}'...")
        try:
            supa_response = (
                self.supabase.table(table_name)
                .upsert(insert_rows, on_conflict="url")
                .execute()
            )
            # Supabase SDK å›å‚³çš„ response æ˜¯ä¸€å€‹ PostgrestAPIResponse ç‰©ä»¶
            print(f"ğŸŸ¢ è³‡æ–™åº«å¯«å…¥æˆåŠŸï¼")
            
        except PostgrestAPIError as e:
            # æ•ç²å¸¸è¦‹çš„ Postgrest éŒ¯èª¤ï¼Œä¾‹å¦‚ä¸»éµè¡çª (url unique é™åˆ¶)
            print(f"ğŸ”´ è³‡æ–™åº«å¯«å…¥å¤±æ•— (Postgrest Error): {e}")
        except Exception as e:
            print(f"ğŸ”´ è³‡æ–™åº«å¯«å…¥å¤±æ•— (æœªçŸ¥éŒ¯èª¤): {e}")

# =======================================================================================
# Buildpacks æœƒæŸ¥æ‰¾é€™å€‹ 'app' ç‰©ä»¶
app = FastAPI()
BASE_PATH = "https://aryhsgsnewsapi.onrender.com/api/scrape-specific-news/?category="
supa_client = SupabaseUploader()
category_list = ["é‡‘è","ç†è²¡","æœŸè²¨","è­‰åˆ¸","ç”¢æ¥­","åœ‹éš›"]

@app.get("/", status_code=200, summary="çˆ¬å–ä¸¦ä¸Šå‚³è²¡ç¶“ç›¸é—œé¡åˆ¥çš„æ–°èæ•¸æ“š")
async def curl_news_and_upload():
    """
    æ¥æ”¶ Web è«‹æ±‚å¾Œï¼Œç•°æ­¥çˆ¬å–æ‰€æœ‰æŒ‡å®šé¡åˆ¥çš„æ–°èæ•¸æ“šï¼Œ
    å°‡æ•¸æ“šè½‰æ›å¾Œï¼Œä¸Šå‚³åˆ° Supabase çš„ 'news' è¡¨æ ¼ä¸­ã€‚
    """
    response_list: List[List[Dict[str, Any]]] = []

    async with httpx.AsyncClient(timeout=100.0) as client:
        try:
            for cate in category_list:
                print(f"æ­£åœ¨çˆ¬å–{cate}é¡æ–‡ç« ...")
                url = BASE_PATH+cate
                response = await client.get(url)
                response.raise_for_status()

                transformed_data = transform_datastruct_and_split(response.json())
                response_list.append(transformed_data)
                print(f"{cate}é¡æ–‡ç« çˆ¬å–å®Œæˆ!!!")

            #all_response = response_list[0]+response_list[1]+response_list[2]+response_list[3]+response_list[4]+response_list[5]
            all_response = sum(response_list, [])
        
            await run_in_threadpool(supa_client.upload_data,news_data=all_response, table_name="news")
            return {
                "message": "æ–°èæ•¸æ“šçˆ¬å–ã€å‘é‡è½‰æ›å’Œä¸Šå‚³æˆåŠŸ",
                "total_records_processed": len(all_response)
            }
            
        except ValueError as e:
            print(f"ğŸ”´ è‡´å‘½éŒ¯èª¤ï¼šç’°å¢ƒè¨­å®šå•é¡Œ: {e}")

        except Exception as e:
            print(f"ğŸ”´ é‹è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")

        except IndexError as e:
            print(f"ğŸ”´ çˆ¬å–æ–°èæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")

        except httpx.HTTPStatusError as e:
            error_message = f"ğŸ”´ å¤–éƒ¨ API è«‹æ±‚å¤±æ•— (ç‹€æ…‹ç¢¼: {e.response.status_code}): {e.response.text}"
            print(error_message)
            return {"error": "HTTP è«‹æ±‚å¤±æ•—", "details": error_message}




    

    